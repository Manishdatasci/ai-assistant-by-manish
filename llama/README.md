# ğŸ’¬ TinyLlama Chatbot using LangChain & Streamlit

A lightweight chatbot built using **TinyLlama** via `langchain-ollama` and Streamlit for a real-time chat interface.

## ğŸš€ Features

- ğŸ’¡ Chat with TinyLlama through a web interface  
- ğŸ“œ Chat history maintained using Streamlit's session state  
- âš¡ Real-time streaming responses from the model  
- ğŸ§  Powered by LangChain and TinyLlama via Ollama  

## ğŸ“‚ File Structure

```
â”œâ”€â”€ chatbot_app.py           # Main Streamlit chatbot application  
â”œâ”€â”€ requirements.txt         # Required dependencies  
â”œâ”€â”€ README.md                # Project documentation  
```

## â–¶ï¸ How to Run

1. **Install Ollama and pull the TinyLlama model**  
   [Get Ollama](https://ollama.com) (required to run TinyLlama locally)

```bash
ollama pull tinyllama
```

2. **Install dependencies:**
```bash
pip install -r requirements.txt
```

3. **Launch the Streamlit app:**
```bash
streamlit run chatbot_app.py
```

## ğŸ› ï¸ Tech Stack

- **Python**
- **Streamlit** â€“ Web UI  
- **LangChain** â€“ Model handling & chat logic  
- **Ollama** â€“ Local LLM runtime  
- **TinyLlama** â€“ Lightweight language model  

---

ğŸ‘¨â€ğŸ’» **Developed by Manish Kumar Rajak**
